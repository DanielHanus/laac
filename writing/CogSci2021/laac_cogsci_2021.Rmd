---
title: "A Longitudinal Study of Great Ape Cognition: Stability, Reliability and the Influence of Individual Characteristics"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Manuel Bohn (manuel\_bohn@eva.mpg.de)}
    \\ {\large \bf Johanna Eckert (johanna\_eckert@eva.mpg.de)}
    \\ {\large \bf Daniel Hanus (hanus@eva.mpg.de)}
    \\ {\large \bf Daniel Haun (haun@eva.mpg.de)}
    \\ Department of Comparative Cultural Psychology, Max Planck Institute for Evolutionary Anthropology,
    \\ Deutscher Platz 6, 04103 Leipzig, Germany}

abstract: >
    Primate cognition research allows us to reconstruct the evolution of human cognition. However, temporal and contextual factors that induce variation in cognitive studies with great apes are poorly understood. Here we report on a longitudinal study where we repeatedly tested a comparatively large sample of great apes (N = 40) with the same set of cognitive measures. We investigated the stability of group-level results, the reliability of individual differences and the relation between cognitive performance and individual-level characteristics. We found results to be relatively stable on a group level. Some, but not all, tasks showed acceptable levels of reliability. Cognitive performance across tasks was not systematically related to any particular individual-level predictor. This study highlights the importance of methodological considerations — especially when studying individual differences -- on the route to building a more robust science of primate cognitive evolution.
    
keywords: >
    Primate Cognition; Stability; Reliability; Individual Differences.
    
output: cogsci2016::cogsci_paper
# final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)
```

```{r, cache = F}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(tidybayes)
library(ggridges)
library(glue)
library(brms)
library(stringr)
library(forcats)
library(ggthemes)
library(ggpubr)
library(reshape)
library(tidyboot)

cor_func <- function(x) {
  x %>%
    corrr::correlate()%>%
    gather(time_point, cor, -rowname)%>%
    mutate(cor = replace(cor, duplicated(cor), NA))%>%
    drop_na(cor)
}
```

```{r}
raw_data <-  read_csv("cogsci_data_trial.csv")
data_task <-  read_csv("cogsci_data_task.csv")
```

# Introduction

Primate cognition research can inform us about the evolution of human cognition. This research has contributed significantly to our understanding of the shared and unique aspects of human cognition [@laland2021understanding]. But, like all other branches of cognitive science, primate cognition research faces some critical challenges: Because cognitive processes cannot be observed directly, they must be inferred from behavior. This kind of inference requires strong methods which specify the link between behavior and cognition. In this paper, we report on a longitudinal study that focuses on the stability, reliability and predictability of great apes' performance in a range of cognitive tasks.

To allow for generalization, study results need to replicate. That is, comparable results should be obtained when applying the same method to a new population of individuals. Psychological science has been riddled with problems of non-replicable results [@open2015estimating]. Animal cognition research shows many of the characteristics that have been identified to yield a low replication rate in other psychological fields [@stevens2017replicability; @farrar2020replications]. Furthermore, replication attempts are rare in animal cognition research [@farrar2020replicomp]. A recent review of experimental primate cognition research between 2014 and 2019 found that only 2 % of studies included a replication [@primates2019collaborative]. Replications are rare, in part because researchers only have access to one sample of study participants and therefore cannot test a new sample. Nevertheless, in such conditions, we can ask a more fundamental question: how *repeatable* are the results of a study. That is if we test the same animals multiple times, do we get similar results? Repeatability could be seen as a pre-condition for replicability. In this study, we investigate the stability of results by repeatedly testing the same sample of great apes on the same tasks. 

One way to explain cognitive evolution is to study how cognitive abilities cluster in different species. This approach needs reliable measures [@volter2018comparative]. Reliability refers to the stability of individual differences as opposed to group-level means. Reliability is paramount if a study’s goal is to relate cognitive performance to individual characteristics or external variables: a measure cannot be stronger related to a second measure than to itself. Recent years have seen an increase of individual differences studies in animal cognition research [@shaw2017cognitive]. In these studies, the reliability of the tasks is rarely assessed. Therefore, it is difficult to say if the absence of a relation between two variables is real or merely a consequence of low reliability. As part of this study, we investigate the re-test reliability of a range of commonly used cognitive tasks for great apes. 

```{r}
# include metaanalytic estimate
time_plot_1 <- data_task%>%
  mutate(species = ifelse(grepl("chimp",group),"chimpanzee", group))%>%
  drop_na(performance)%>%
  mutate(time_point = factor(time_point))%>%
  group_by(task, time_point, species)%>%
  summarise(mean = mean(performance,na.rm = TRUE))

switch_data <- raw_data%>%
  filter(grepl("switch", task))%>%
  group_by(task, time_point)%>%
  tidyboot_mean(col = code, na.rm = TRUE)%>%
  mutate(phase = str_remove(task,"switching_"),
         task = "switching")

overview_plot <- data_task%>%
  mutate(species = ifelse(grepl("chimp",group),"chimpanzee", group))%>%
  mutate(time_point = factor(time_point))%>%
  group_by(task, performance)%>%
  mutate(n = n(),
         chance = ifelse(task == "switching", 1/3, ifelse(task == "gaze_following", NA,0.5)))%>%
  ungroup()

time_plot_2 <- data_task%>%
  mutate(species = ifelse(grepl("chimp",group),"chimpanzee", group))%>%
  drop_na(performance)%>%
  mutate(time_point = factor(time_point))%>%
  group_by(task, time_point)%>%
  tidyboot_mean(col = performance, na.rm = TRUE)

perf_plot <- ggplot()+
  facet_grid(~task)+
  geom_line(data = overview_plot, aes(x = time_point, y = performance, group = subject, size = n),col = "grey",alpha = .2)+
  geom_hline(data = overview_plot,aes(yintercept = chance), lty = 2, col = "black", alpha = .75)+
  geom_point(data = time_plot_1, aes(x = time_point, y = mean,  col = species), alpha = .6)+
  geom_line(data = time_plot_1, aes(x = time_point, y = mean, col = species, group = species), alpha = .6)+
  geom_pointrange(data = time_plot_2, aes(x = time_point, y = mean, ymin = ci_lower, ymax = ci_upper),pch = 4)+
  geom_line(data = time_plot_2, aes(x = time_point, y = mean, group = task))+
  theme_minimal()+
    geom_pointrange(data = switch_data, aes(x = time_point, y = mean, ymin = ci_lower, ymax = ci_upper, pch = phase), alpha = .5)+
  labs(x = "Time Point", y = "Performance")+
  geom_line(data = switch_data, aes(x = time_point, y = mean, group = phase), alpha = .5)+
  theme_minimal(base_size = 8)+
  guides(size = F)+
  scale_shape(name = "Switching Phase")+
  scale_color_ptol(name = "Group")
```

```{r perfplot, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=2.25, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Results from the five cognitive tasks across time points. Black crosses show mean performance at each time point across species (with 95\\% CI). Colored dots show mean performance by species. Transparent grey lines connect individual performances across time points, with the line’s width corresponding to the number of participants. Dashed line shows the chance level inference whenever applicable. The panel for switching includes triangles and dots showing the mean performance in the two phases from which the overall performance score was computed (see main text)."}
perf_plot
```

Researchers in animal cognition often assume that performance in cognitive tasks can (in part) be explained by individual-level characteristics such as age, sex, rank or rearing history. In many cases, such predictors are included without a specific hypothesis, either to control for potential effects or because they are implicitly assumed to influence cognitive performance in general. Habitually including these predictors without a theoretical indication is problematic because -- in combination with selective reporting -- it may increase the rate of false-positive results [@simmons2011false]. As part of the study reported here, we investigated whether individual characteristics influence cognitive performance on a broad scale.

In the following, we describe the first results from a longitudinal study with great apes. We ask how stable performance is on a group level, how reliable individual differences are and to what extent these individual differences can be explained by a common set of predictors. We chose five tasks that cover a broad range of cognitive abilities: causal inference, inference by exclusion, gaze following, quantity discrimination, and switching flexibility. We tested a sample of individuals from four great ape species: Bonobos (*Pan paniscus*, Chimpanzees (*Pan troglodytes*), Gorillas (*Gorilla gorilla* and Orangutans(*Pongo abelii*) on regular intervals.

# Methods

## Participants
```{r}
participants <- raw_data%>%
  mutate(species = ifelse(grepl("chimp", group),"chimpanzee",group))%>%
  group_by(species)%>%
  mutate(minage = round(min(age),1),
         maxage = round(max(age),1))%>%
  group_by(species, sex, minage,maxage)%>%
  summarise(n = length(unique(subject)))
  
  
 tpn <- raw_data%>%
  group_by(time_point)%>%
  summarise(n = length(unique(subject)))
```
A total of `r sum(participants$n)` great apes participated at least once in one of the tasks. This included `r participants%>%filter(species == "bonobo")%>%pull(n)%>%sum()` Bonobos (`r participants%>%filter(species == "bonobo", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "bonobo", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "bonobo", sex == "f")%>%pull(maxage)`), `r participants%>%filter(species == "chimpanzee")%>%pull(n)%>%sum()` Chimpanzees (`r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(maxage)`), `r participants%>%filter(species == "gorilla")%>%pull(n)%>%sum()` Gorillas (`r participants%>%filter(species == "gorilla", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "gorilla", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "gorilla", sex == "f")%>%pull(maxage)`), and `r participants%>%filter(species == "gorilla")%>%pull(n)%>%sum()` Orangutans (`r participants%>%filter(species == "orangutan", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "orangutan", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "orangutan", sex == "f")%>%pull(maxage)`). The sample size at the different time points ranged from `r min(tpn$n)` to `r max(tpn$n)`.

Apes were housed at the [masked for peer review]. They lived in groups, with one group per species and two chimpanzee groups. Research was noninvasive and strictly adhered to the legal requirements in Germany. Animal husbandry and research complied with the European Association of Zoos and Aquaria Minimum Standards for the Accommodation and Care of Animals in Zoos and Aquaria as well as the World Association of Zoos and Aquariums Ethical Guidelines for the Conduct of Research on Animals by Zoos and Aquariums. Participation was voluntary, all food was given in addition to the daily diet, and water was available ad libitum throughout the study. The study was approved by an internal ethics committee at the [masked for peer review].

## Design, Setup and Procedure
```{r}
# bm_stab_data <- raw_data%>%
#   mutate(trial_time_point = scale(trial_time_point),
#          time_point = factor(time_point))%>%
#   group_split(task, .keep = TRUE)%>%
#   as.list()
# 
# # bm_stab <- brm_multiple(code ~  trial_time_point + (trial_time_point|group/subject) + (1|time_point),
# #             family = bernoulli(),
# #             data = bm_stab_data,
# #             combine = F,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_stab,file = "saves/bm_stab.rds")
# 
# bm_stab <- readRDS(file = "saves/bm_stab.rds")
# 
# bm_stab_switch_data <- data_task%>%
#   filter(task == "switching")%>%
#   mutate(time_point = factor(time_point))%>%
#   filter(!is.na(performance))
# 
# # bm_stab_switch <- brm(performance | trunc(lb=-1,ub=1) ~  1 + (1|group/subject) + (1|time_point),
# #                       family = gaussian(),
# #                       data = bm_stab_switch_data,
# #                       control = list(adapt_delta = 0.99, max_treedepth = 20),
# #                       iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_stab_switch,file = "saves/bm_stab_switch.rds")
# # 
# bm_stab_switch <- readRDS(file = "saves/bm_stab_switch.rds")
# 
# post_sample_task <- bind_rows(
# posterior_samples(bm_stab[1], pars = c("b_Intercept","r_time_point.1.Intercept.","r_time_point.2.Intercept.","r_time_point.3.Intercept.","r_time_point.4.Intercept.","r_time_point.5.Intercept.","r_time_point.6.Intercept.","r_time_point.7.Intercept.","r_time_point.8.Intercept."))%>%
#   gather(time_point, tp_int, -b_Intercept)%>%
#   mutate(b_Intercept = tp_int + b_Intercept, 
#          time_point = parse_number(str_remove(time_point,"\\.")))%>%mutate(task = "causality"),
# 
# posterior_samples(bm_stab[2], pars = c("b_Intercept","r_time_point.1.Intercept.","r_time_point.2.Intercept.","r_time_point.3.Intercept.","r_time_point.4.Intercept.","r_time_point.5.Intercept.","r_time_point.6.Intercept.","r_time_point.7.Intercept.","r_time_point.8.Intercept."))%>%
#   gather(time_point, tp_int, -b_Intercept)%>%
#   mutate(b_Intercept = tp_int + b_Intercept, 
#          time_point = parse_number(str_remove(time_point,"\\.")))%>%mutate(task = "gaze_following"),
# 
# posterior_samples(bm_stab[3], pars = c("b_Intercept","r_time_point.1.Intercept.","r_time_point.2.Intercept.","r_time_point.3.Intercept.","r_time_point.4.Intercept.","r_time_point.5.Intercept.","r_time_point.6.Intercept.","r_time_point.7.Intercept.","r_time_point.8.Intercept."))%>%
#   gather(time_point, tp_int, -b_Intercept)%>%
#   mutate(b_Intercept = tp_int + b_Intercept, 
#          time_point = parse_number(str_remove(time_point,"\\.")))%>%mutate(task = "inference"),
# 
# posterior_samples(bm_stab[4], pars = c("b_Intercept","r_time_point.1.Intercept.","r_time_point.2.Intercept.","r_time_point.3.Intercept.","r_time_point.4.Intercept.","r_time_point.5.Intercept.","r_time_point.6.Intercept.","r_time_point.7.Intercept.","r_time_point.8.Intercept."))%>%
#   gather(time_point, tp_int, -b_Intercept)%>%
#   mutate(b_Intercept = tp_int + b_Intercept, 
#          time_point = parse_number(str_remove(time_point,"\\.")))%>%mutate(task = "quantity"),
# 
# posterior_samples(bm_stab_switch, pars = c("b_Intercept","r_time_point.1.Intercept.","r_time_point.2.Intercept.","r_time_point.3.Intercept.","r_time_point.4.Intercept.","r_time_point.5.Intercept.","r_time_point.6.Intercept.","r_time_point.7.Intercept.","r_time_point.8.Intercept."))%>%
#   gather(time_point, tp_int, -b_Intercept)%>%
#   mutate(b_Intercept = tp_int + b_Intercept, 
#          time_point = parse_number(str_remove(time_point,"\\.")))%>%mutate(task = "switching")
# )%>%
#   group_by(task,time_point)%>%
#   filter(!is.na(time_point))%>%
#   mutate(mean = mean(b_Intercept),
#          #time_point = paste("Time point ", time_point, sep = "")
#          )%>%
#   mutate(time_point = factor(time_point))
# 
# pooled_effect_task <- bind_rows(
# posterior_samples(bm_stab[1])%>%select(b_Intercept)%>%mutate(task = "causality", time_point = "Pooled Effect"),
# posterior_samples(bm_stab[2])%>%select(b_Intercept)%>%mutate(task = "gaze_following", time_point = "Pooled Effect"),
# posterior_samples(bm_stab[3])%>%select(b_Intercept)%>%mutate(task = "inference", time_point = "Pooled Effect"),
# posterior_samples(bm_stab[4])%>%select(b_Intercept)%>%mutate(task = "quantity", time_point = "Pooled Effect"),
# posterior_samples(bm_stab_switch)%>%select(b_Intercept)%>%mutate(task = "switching", time_point = "Pooled Effect")
# )%>%
#   mutate(time_point = factor(time_point))
# 
# forest_data_task <- bind_rows(post_sample_task, pooled_effect_task)%>%
#   mutate(time_point = factor(time_point),
#          measure = "Model Intercept",
#          task = factor(task))%>%
#   filter(task != "switching_place")
# 
# saveRDS(forest_data_task,file = "saves/forest_data_task.rds")
 
forest_data_task <- readRDS(file = "saves/forest_data_task.rds")

# forest_data_task_summary <- forest_data_task %>%
#   group_by(task,measure, time_point) %>% 
#   mean_hdci(b_Intercept)
# 
# saveRDS(forest_data_task_summary,file = "saves/forest_data_task_summary.rds")
#  
forest_data_task_summary <- readRDS(file = "saves/forest_data_task_summary.rds")

## sd across tasks

# sd_sample_task <- bind_rows(
#   posterior_samples(bm_stab[1], pars = c("sd_time_point__Intercept"))%>%mutate(task = "causality")%>%
#     mutate(tau_squared = sd_time_point__Intercept^2),
#   posterior_samples(bm_stab[2], pars = c("sd_time_point__Intercept"))%>%mutate(task = "gaze_following")%>%
#     mutate(tau_squared = sd_time_point__Intercept^2),
#   posterior_samples(bm_stab[3], pars = c("sd_time_point__Intercept"))%>%mutate(task = "inference")%>%
#     mutate(tau_squared = sd_time_point__Intercept^2),
#   posterior_samples(bm_stab[4], pars = c("sd_time_point__Intercept"))%>%mutate(task = "quantity")%>%
#     mutate(tau_squared = sd_time_point__Intercept^2),
#   posterior_samples(bm_stab_switch, pars = c("sd_time_point__Intercept"))%>%mutate(task = "switching")%>%
#     mutate(tau_squared = sd_time_point__Intercept^2)
# )%>%
#   mutate(time_point = "Tau")%>%
#   filter(task != "switching_place")%>%
#   mutate(time_point = factor(time_point),
#          measure = "Heterogeneity")
# 
# saveRDS(sd_sample_task,file = "saves/sd_sample_task.rds")
 
sd_sample_task <- readRDS(file = "saves/sd_sample_task.rds")

# sd_task_summary <- sd_sample_task %>%
#   group_by(measure, time_point,task) %>% 
#   mean_hdci(sd_time_point__Intercept,tau_squared)
# 
# 
# saveRDS(sd_task_summary,file = "saves/sd_task_summary.rds")
 
sd_task_summary <- readRDS(file = "saves/sd_task_summary.rds")


annotation_custom2 <- function (grob, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, data) 
{
  layer(data = data, stat = StatIdentity, position = PositionIdentity, 
        geom = ggplot2:::GeomCustomAnn,
        inherit.aes = F, params = list(grob = grob, 
                                          xmin = xmin, xmax = xmax, 
                                          ymin = ymin, ymax = ymax))
}

get_inset <- function(df){
  p <- ggplot(aes(b_Intercept, relevel(time_point, "Pooled Effect", after = Inf)), 
       data = df) +
  geom_segment(y = "1", yend = "1", x = -Inf, xend = Inf, color = "black", size = .5) +
  geom_vline(xintercept = 0, color = "black", size = .5, lty = 2) +
  geom_density_ridges(rel_min_height = 0.01,col = "black", scale = 3,
                      alpha = 0.25) +
  labs(x = element_blank(),
       y = element_blank()) +
  theme_few(base_size = 6)+
  guides(fill = F)+
  scale_fill_colorblind()+
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
  return(p)
}

insets <- forest_data_task %>% 
  base::split(f = .$task) %>%
  purrr::map(~annotation_custom2(
    grob = ggplotGrob(get_inset(.) + scale_fill_colorblind()), 
    data = data.frame(task=unique(.$task)),
    xmin = 0.2, xmax = 1.8, ymin = 5, ymax = 17.5)
  )


meta_plot <- ggplot(aes(b_Intercept, fill = task), 
       data = forest_data_task) +
  facet_grid(~task, scales = "free_y")+
  geom_vline(xintercept = 0, color = "black", size = .5, lty = 2) +
  geom_text(data = mutate_if(sd_task_summary, is.numeric, round, 2),
     aes(label = glue("{sd_time_point__Intercept} [{sd_time_point__Intercept.lower}, {sd_time_point__Intercept.upper}]"), x= Inf, y = 1), hjust = "inward", size = 2)+
  geom_density(data = sd_sample_task, aes(sd_time_point__Intercept ,fill = task),col = "black", alpha = 0.5) +
  xlim(0,2)+
  labs(x = element_blank(),
       y = element_blank()) +
  theme_minimal(base_size = 8)+
  guides(fill = F)+
  scale_fill_colorblind()

p_meta <- meta_plot + insets
```

```{r metaplot, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=1.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Posterior distributions for $\\tau$ from the meta-analytic models for each task. Numbers denote mean and 95\\% HDI for $\\tau$. Insets show the posterior distribution for the model intercept estimate at each time point and the overall estimate at the bottom (separated by the black line)."}
p_meta
```

We tested apes on the same five tasks every other week. Here we report the data from the first eight time points. The tasks were presented in the same order and with the same positioning and counterbalancing (to keep conditions constant between individuals and across occasions). Apes were tested in familiar sleeping or observation rooms by a single experimenter. Whenever possible, they were tested individually. For each individual, the tasks at one time point were usually spread out across two consecutive days with causality and inference on day 1 and quantity and switching on day 2. Gaze following trials were run at the beginning and the end of each day. The basic setup comprised a sliding table positioned in front of a clear Plexiglas panel with three holes in it. The experimenter sat on a small stool and used an occluder to cover the sliding table.

### Causality

The causality and inference tasks were modeled after [@call2004inferences]. Two identical cups with a lid were placed left and right on the table. The experimenter covered the table with the occluder, retrieved a piece of food, showed it to the ape, and hid it in one the cups outside the participant’s view. Next, they removed the occluder, picked up the baited cup and shook it three times, which produced a rattling sound. Next, the cup was put back in place, the sliding table pushed forwards, and the participant made a choice by pointing to one of the cups. If they picked the baited cup, their choice was coded as correct, and they received the reward. If they chose the other cup, they did not. On each time point, participants received 12 trials. 

### Inference

Inference trials were identical to causality trials, but instead of shaking the baited cup, the experimenter shook the empty cup. On each time point, participants received 12 trials. Inference trials were intermixed with causality trials.

### Gaze Following

The gaze following task was modeled after [@brauer2005all]. The experimenter sat opposite the ape and handed over food at a constant pace. That is, the experimenter picked up a piece of food, briefly held it out in front of her face and then handed it over to the participant. At some point, the experimenter looked up (i.e., moving her head up) while holding up the food in front of her head. After 10s, the experimenter looked down again and handed over the food. We coded whether the subject looked up during the 10s interval. Participants received a total of 8 trials, spread out across the two test days.    

### Quantity Discrimination

For this task, we followed the general procedure of [@hanus2007discrete]. Two small plates were presented left and right on the table. The experimenter placed 5 small food pieces on one plate and 7 on the other. Then they pushed the sliding table forwards, and the subject made a choice. We coded as correct when the subject chose the plate with the larger quantity. There were 12 trials per time point.   

### Switching

This task was modeled after [@haun2006evolutionary]. Three differently looking cups (metal cup with handle, red plastic ice cone, red cup without handle) were placed next to each other on the table. There were two conditions. In the place condition, the experimenter hid a piece of food under one of the cups in full view of the participant. Next, the cups were covered by the occluder and the experimenter switched the position of two cups, while the reward remained in the same location. We coded as correct if the participant chose the location where the food was hidden. Participants received four trials in this condition. The place condition was run first. The feature condition followed the same procedure, but now the experimenter also moved the reward when switching the cups. The switch between conditions happened without informing the participant in any way. A correct choice in this condition meant choosing the location to which the cup plus the food were moved. Here, participants received eight trials. The dependent measure of interest for this task was calculated as: `[proportion correct place] - (1 - [proportion correct feature])`.  Positive values in this score mean that participants could quickly switch from choosing based on location to choosing based on feature. High negative values suggest that participants did not or hardly switch strategies.   

# Analysis and Results

We combined the data from all species for the analysis because sample sizes for some species were too small to get representative estimates. However, we accounted for the nesting of subjects in species as part of the random effect structure of our models. All analyses were run in `R` [@R-base]. Bayesian multilevel models were implemented using the package `brms` [@R-brms_a] and default priors. All models included random intercepts for participants nested within group and random slopes for trial (`trial|group/subject`). Data and analysis code can be found in the associate online repository (see below). 

## Stability

First, we looked at group-level stability in performance. That is, we asked how much performance varied across time points in the different tasks. For this analysis, we ignored the temporal order of the different time points and treated them as repetitions of the same experiment (i.e., time point was treated as a factor instead of a numerical variable). As such, we asked a meta-analytic question: how much variation is there between different instances of the same experiment? To answer this, we fitted a mixed model with a random intercept term for time point to the data from each task^[We modeled the trial by trial data using a binomial distribution in a logistic GLMM for all tasks, except switching. Here we modeled the score (by time point) as a truncated normal distribution. As mentioned above, these models included random intercept terms for individuals nested within groups]. As part of each model, we estimated a standard deviation of the random intercept term ($\tau$), which reflects the variation between time points. 

Figure \ref{fig:perfplot} visualizes performance across time points. For causality, inference and quantity, we can evaluate group-level performance by comparing it to chance (50% correct = intercept of 0 in link space). Group-level performance was reliably above chance for causality and quantity but at chance for inference. There is no such reference level for gaze following, and we can simply say that at least some individuals of all species followed the experimenter's gaze. The switching score was consistently negative, suggesting that - on a group level - apes did not switch strategies.    

Figure \ref{fig:metaplot} shows the posterior distribution of $\tau$ for each task. While performance was very stable for inference, quantity and switching ($\tau$ very close to 0), performance was slightly more variable for causality and varied substantially for gaze following. For causality, variation did not seem to follow a clear temporal pattern. On the other hand, for gaze following, there seems to be a downward trend with apes (as a group) becoming less likely to follow the experimenter's gaze. We explore this temporal pattern in more detail below. Taken together, we may say that 4 out of 5 measures yield stable measures of group-level performance. For inference, however, stability corresponds to a stable performance at chance level, which suggests that the task was rather difficult. Whether that meant that participants simply guessed on each trial, we will explore in the next section.

## Reliability 

Next, we asked how stable performance was on an individual level. This question also relates to each task’s reliability - how well suited it is to capture differences between individuals. In general, reliability is high if individuals are consistently ranked across measurement instances. One way to assess reliability is to correlate performance from two time points (re-test reliability). Because we had multiple time points, we computed pairwise correlations for all combinations of time points (total of 28 unique correlations per task). This resulted in a distribution of correlations, which we visualize in Figure \ref{fig:relplot}. Results suggest good re-test reliability for gaze following, causality and inference, variable reliability for quantity and poor reliability for switching. This pattern is interesting in light of the group-level performance we reported above: stable performance on a group level (stability) does not imply stable individual differences (reliability). We come back to this point in the discussion. 

```{r}
cor_data <- data_task%>%
  drop_na(performance)%>%
  select(subject, task, time_point, performance)%>%
  pivot_wider(values_from = "performance", names_from = "time_point")%>%
  ungroup()%>%
  select(-subject)%>%
  group_by(task)%>%
  group_split(.keep = F)%>%
  setNames(unique(data_task$task))%>%
  purrr::map(cor_func)%>%
  melt()%>%
  mutate(task = L1)%>%
  select(task, value)

cor_data_sum <- cor_data%>%
  group_by(task)%>%
  mean_hdci(value)
  
# ggplot(cor_data, aes(x = value, col = task, fill = task))+
#   #geom_histogram(col = "black", fill = "white")+
#   geom_density(alpha = .3)+
#   #xlim(-0.5,1)+
#   #facet_wrap(~task)+
#   labs(x = "Correlation Coefficient", y = "")+
#   theme_minimal()+
#   scale_color_colorblind(name = "Task")+
#   scale_fill_colorblind(name = "Task")


rel_plot <- ggplot(data = cor_data, aes(x = value , y = task ,fill = task)) +
  geom_vline(xintercept = 0, color = "black", size = .5, lty = 2) +
  geom_density_ridges(rel_min_height = 0.01,col = "black", scale = 2,
                      alpha = 0.5) +
  geom_pointintervalh(data = cor_data_sum, size = 1)+
  labs(x = "Correlation Coefficient",
       y = element_blank()) +
  geom_text(data = mutate_if(cor_data_sum, is.numeric, round, 2),
    aes(label = glue("{value} [{.lower}, {.upper}]"), x= Inf), hjust = "inward", size = 2)+
  theme_minimal(base_size = 10)+
  xlim(-0.5,1.5)+
  guides(fill = F)+
  scale_fill_colorblind()
```

```{r relplot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=3.4, fig.height=2, fig.cap = "Distribution of correlations between time points for each task. Dots represent the mean of the distribution with 95\\% HDI. Numbers denote mean and 95\\% HDI." }

rel_plot

```

## Predictors
```{r}
# # bm_model_data <- raw_data%>%
# #   filter(!is.na(rank) & !is.na(age) & !is.na(sex) & !is.na(rearing))%>%
# #   mutate(trial_time_point = scale(trial_time_point),
# #          time_point = scale(time_point),
# #          rank = scale(rank),
# #          age = scale(age))%>%
# #   group_split(task, .keep = TRUE)%>%
# #   as.list()
# # 
# #   
# # bm_null <- brm_multiple(code ~ trial_time_point + time_point + (trial_time_point + time_point|group/subject),
# #             family = bernoulli(),
# #             data = bm_model_data,
# #             combine = F,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_null,file = "saves/bm_null.rds")
# # 
# bm_null <- readRDS(file = "saves/bm_null.rds")
# # 
# # bm_rank <- brm_multiple(code ~ rank + trial_time_point + time_point + (trial_time_point + time_point|group/subject),
# #             family = bernoulli(),
# #             data = bm_model_data,
# #             combine = F,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_rank,file = "saves/bm_rank.rds")
# # 
# bm_rank <- readRDS(file = "saves/bm_rank.rds")
# # 
# # bm_age <- brm_multiple(code ~ age + trial_time_point + time_point + (trial_time_point + time_point|group/subject),
# #             family = bernoulli(),
# #             data = bm_model_data,
# #             combine = F,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_age,file = "saves/bm_age.rds")
# # 
# bm_age <- readRDS(file = "saves/bm_age.rds")
# # 
# # bm_sex <- brm_multiple(code ~ sex + trial_time_point + time_point + (trial_time_point + time_point|group/subject),
# #             family = bernoulli(),
# #             data = bm_model_data,
# #             combine = F,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_sex,file = "saves/bm_sex.rds")
# # 
# bm_sex <- readRDS(file = "saves/bm_sex.rds")
# # 
# # bm_rearing <- brm_multiple(code ~ rearing + trial_time_point + time_point + (trial_time_point + time_point|group/subject),
# #             family = bernoulli(),
# #             data = bm_model_data,
# #             combine = F,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_rearing,file = "saves/bm_rearing.rds")
# 
# bm_rearing <- readRDS(file = "saves/bm_rearing.rds")
# 
# ## switching 
# 
# # bm_switch_data <- data_task%>%
# #   filter(task == "switching")%>%
# #   left_join(raw_data%>%filter(!is.na(rank))%>%group_by(subject,time_point,rank)%>%summarise(rank = mean(rank)))%>%
# #   filter( !is.na(rank) & !is.na(age) & !is.na(sex) & !is.na(rearing) & !is.na(performance))%>%
# #   mutate(time_point = scale(time_point),
# #          rank = scale(rank),
# #          age = scale(age))
# # 
# # bm_switch_null <- brm(performance | trunc(lb=-1,ub=1) ~  time_point + (time_point|group/subject),
# #             family = gaussian(),
# #             data = bm_switch_data,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# #saveRDS(bm_switch_null,file = "saves/bm_switch_null.rds")
# 
# bm_switch_null <- readRDS(file = "saves/bm_switch_null.rds")
# 
# # bm_switch_rank <- brm(performance | trunc(lb=-1,ub=1) ~  rank + time_point + (time_point|group/subject),
# #             family = gaussian(),
# #             data = bm_switch_data,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# #saveRDS(bm_switch_rank,file = "saves/bm_switch_rank.rds")
# 
# bm_switch_rank <- readRDS(file = "saves/bm_switch_rank.rds")
# 
# # bm_switch_sex <- brm(performance | trunc(lb=-1,ub=1) ~  sex + time_point + (time_point|group/subject),
# #             family = gaussian(),
# #             data = bm_switch_data,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_switch_sex,file = "saves/bm_switch_sex.rds")
# 
# bm_switch_sex <- readRDS(file = "saves/bm_switch_sex.rds")
# 
# # bm_switch_age <- brm(performance | trunc(lb=-1,ub=1) ~  age + time_point + (time_point|group/subject),
# #             family = gaussian(),
# #             data = bm_switch_data,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_switch_age,file = "saves/bm_switch_age.rds")
# 
# bm_switch_age <- readRDS(file = "saves/bm_switch_age.rds")
# 
# # bm_switch_rearing <- brm(performance | trunc(lb=-1,ub=1) ~  rearing + time_point + (time_point|group/subject),
# #             family = gaussian(),
# #             data = bm_switch_data,
# #             control = list(adapt_delta = 0.99, max_treedepth = 20),
# #             iter = 5000, cores = 3, chains = 3)
# # 
# # saveRDS(bm_switch_rearing,file = "saves/bm_switch_rearing.rds")
# 
# bm_switch_rearing <- readRDS(file = "saves/bm_switch_rearing.rds")
```

```{r}

# weights_caus <- model_weights(
#   bm_null%>%purrr::simplify() %>% pluck(1)%>%add_criterion(criterion = "waic"),
#   bm_rank%>%purrr::simplify() %>% pluck(1)%>%add_criterion(criterion = "waic"),
#   bm_age%>%purrr::simplify() %>% pluck(1)%>%add_criterion(criterion = "waic"),
#   bm_sex%>%purrr::simplify() %>% pluck(1)%>%add_criterion(criterion = "waic"),
#   bm_rearing%>%purrr::simplify() %>% pluck(1)%>%add_criterion(criterion = "waic"),
#   weights = "waic")%>%
#   as_tibble()%>%
#   dplyr::rename(weight = value)%>%
#   mutate(model = c("baseline","rank","age","sex","rearing"),
#          task = "causality")
# 
# 
# weights_gaze <- model_weights(
#   bm_null%>%purrr::simplify() %>% pluck(2)%>%add_criterion(criterion = "waic"),
#   bm_rank%>%purrr::simplify() %>% pluck(2)%>%add_criterion(criterion = "waic"),
#   bm_age%>%purrr::simplify() %>% pluck(2)%>%add_criterion(criterion = "waic"),
#   bm_sex%>%purrr::simplify() %>% pluck(2)%>%add_criterion(criterion = "waic"),
#   bm_rearing%>%purrr::simplify() %>% pluck(2)%>%add_criterion(criterion = "waic"),
#   weights = "waic")%>%
#   as_tibble()%>%
#   dplyr::rename(weight = value)%>%
#   mutate(model = c("baseline","rank","age","sex","rearing"),
#          task = "gaze_following")
# 
# 
# weights_inf <- model_weights(
#   bm_null%>%purrr::simplify() %>% pluck(3)%>%add_criterion(criterion = "waic"),
#   bm_rank%>%purrr::simplify() %>% pluck(3)%>%add_criterion(criterion = "waic"),
#   bm_age%>%purrr::simplify() %>% pluck(3)%>%add_criterion(criterion = "waic"),
#   bm_sex%>%purrr::simplify() %>% pluck(3)%>%add_criterion(criterion = "waic"),
#   bm_rearing%>%purrr::simplify() %>% pluck(3)%>%add_criterion(criterion = "waic"),
#   weights = "waic")%>%
#   as_tibble()%>%
#   dplyr::rename(weight = value)%>%
#   mutate(model = c("baseline","rank","age","sex","rearing"),
#          task = "inference")
# 
# weights_quant <- model_weights(
#   bm_null%>%purrr::simplify() %>% pluck(4)%>%add_criterion(criterion = "waic"),
#   bm_rank%>%purrr::simplify() %>% pluck(4)%>%add_criterion(criterion = "waic"),
#   bm_age%>%purrr::simplify() %>% pluck(4)%>%add_criterion(criterion = "waic"),
#   bm_sex%>%purrr::simplify() %>% pluck(4)%>%add_criterion(criterion = "waic"),
#   bm_rearing%>%purrr::simplify() %>% pluck(4)%>%add_criterion(criterion = "waic"),
#   weights = "waic")%>%
#   as_tibble()%>%
#   dplyr::rename(weight = value)%>%
#   mutate(model = c("baseline","rank","age","sex","rearing"),
#          task = "quantity")
# 
# weights_switch <- model_weights(
#   bm_switch_null%>%add_criterion(criterion = "waic"),
#   bm_switch_rank%>%add_criterion(criterion = "waic"),
#   bm_switch_age%>%add_criterion(criterion = "waic"),
#   bm_switch_sex%>%add_criterion(criterion = "waic"),
#   bm_switch_rearing%>%add_criterion(criterion = "waic"),
#   weights = "waic")%>%
#   as_tibble()%>%
#   dplyr::rename(weight = value)%>%
#   mutate(model = c("baseline","rank","age","sex","rearing"),
#          task = "switching")
# 
# 
# model_weights <- bind_rows(
#   weights_caus,
#   weights_gaze,
#   weights_inf,
#   weights_quant,
#   weights_switch
# )
# 
# saveRDS(model_weights, file = "saves/model_weights.rds")

model_weights <-  readRDS(file = "saves/model_weights.rds")
```

```{r}
# caus_waic <- bind_rows(
# 
#   as_tibble(waic(bm_null%>%purrr::simplify() %>% pluck(1))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "baseline", task = "causality"),
#     as_tibble(waic(bm_rank%>%purrr::simplify() %>% pluck(1))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rank", task = "causality"),
#     as_tibble(waic(bm_age%>%purrr::simplify() %>% pluck(1))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "age", task = "causality"),
#     as_tibble(waic(bm_sex%>%purrr::simplify() %>% pluck(1))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "sex", task = "causality"),
#     as_tibble(waic(bm_rearing%>%purrr::simplify() %>% pluck(1))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rearing", task = "causality")
# )
# 
# gaze_waic <- bind_rows(
# 
#   as_tibble(waic(bm_null%>%purrr::simplify() %>% pluck(2))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "baseline", task = "gaze_following"),
#     as_tibble(waic(bm_rank%>%purrr::simplify() %>% pluck(2))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rank", task = "gaze_following"),
#     as_tibble(waic(bm_age%>%purrr::simplify() %>% pluck(2))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "age", task = "gaze_following"),
#     as_tibble(waic(bm_sex%>%purrr::simplify() %>% pluck(2))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "sex", task = "gaze_following"),
#     as_tibble(waic(bm_rearing%>%purrr::simplify() %>% pluck(2))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rearing", task = "gaze_following")
# )
# 
# inf_waic <- bind_rows(
# 
#     as_tibble(waic(bm_null%>%purrr::simplify() %>% pluck(3))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "baseline", task = "inference"),
#     as_tibble(waic(bm_rank%>%purrr::simplify() %>% pluck(3))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rank", task = "inference"),
#     as_tibble(waic(bm_age%>%purrr::simplify() %>% pluck(3))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "age", task = "inference"),
#     as_tibble(waic(bm_sex%>%purrr::simplify() %>% pluck(3))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "sex", task = "inference"),
#     as_tibble(waic(bm_rearing%>%purrr::simplify() %>% pluck(3))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rearing", task = "inference")
# 
# )
# 
# quant_waic <- bind_rows(
# 
#       as_tibble(waic(bm_null%>%purrr::simplify() %>% pluck(4))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "baseline", task = "quantity"),
#     as_tibble(waic(bm_rank%>%purrr::simplify() %>% pluck(4))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rank", task = "quantity"),
#     as_tibble(waic(bm_age%>%purrr::simplify() %>% pluck(4))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "age", task = "quantity"),
#     as_tibble(waic(bm_sex%>%purrr::simplify() %>% pluck(4))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "sex", task = "quantity"),
#     as_tibble(waic(bm_rearing%>%purrr::simplify() %>% pluck(4))$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rearing", task = "quantity")
# 
# )
# 
# switch_waic <- bind_rows(
# 
#       as_tibble(waic(bm_switch_null)$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "baseline", task = "switching"),
#     as_tibble(waic(bm_switch_rank)$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rank", task = "switching"),
#     as_tibble(waic(bm_switch_age)$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "age", task = "switching"),
#     as_tibble(waic(bm_switch_sex)$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "sex", task = "switching"),
#     as_tibble(waic(bm_switch_rearing)$estimates, rownames = "measure")%>%filter(measure == "waic")%>%mutate(model = "rearing", task = "switching")
# 
# )
# 
# model_waic <- bind_rows(
#   caus_waic,
#   gaze_waic,
#   inf_waic,
#   quant_waic,
#   switch_waic
# )%>%
#   dplyr::rename(waic = Estimate,
#          se_waic = SE)%>%
#   select(-measure)
# 
# saveRDS(model_waic, file = "saves/model_waic.rds")

model_waic <-  readRDS(file = "saves/model_waic.rds")         
```

```{r}
model_comp <- model_waic %>%left_join(model_weights)

# comp_plot <- ggplot(data = model_comp, aes(fill=model, y=weight, x = task, label = paste(waic%>%round(digits =1), se_waic%>%round(digits =1), sep = "\n")))+
#   geom_bar(position="stack", stat="identity", width = .5, alpha = .8)+
#   geom_text(size = 2, position = position_stack(vjust = 0.5))+
#   labs(x = "", y = "WAIC Model Weights")+
#   scale_fill_ptol(name = "Predictor")+
#   theme_minimal()+
#   coord_flip()+
#   theme(legend.position = "right")
```

In the final set of analysis, we investigated if variation in cognitive performance could -- in part -- be explained by participant characteristics. We chose to look at variables that are commonly analyzed in the primate cognition literature: age, sex, rank and rearing history. Rank was rated by animal keepers at every time point, and rearing history was classified as "mother reared", "human reared" or "unknown".

For each task, we ran the same five models^[We used the same response distributions as in the stability analysis.]: A baseline model predicting performance by time point (numerical) and trial as well as four models, each with one of the predictors (age, sex, rank and rearing history), added to the baseline model. We did not investigate any interaction models (interactions among the predictors or with time point) because we had no specific hypothesis in that direction. We used Bayesian model comparison based on WAIC (widely applicable information criterion) scores and weights [@rethinking]. This comparison tells us which of the models considered makes the best out-of-sample predictions. If the model with one predictor (e.g., age) were consistently assigned the highest weight across tasks, we would conclude that participants’ age best predicts cognitive performance.

Table 1 gives WAIC scores and weights for each model and task. Figure \ref{fig:predplot} shows the posterior distribution of the test predictors (as well as for time point). The baseline model was ranked highest across tasks (first or second for all tasks), suggesting that none of the test predictors was consistently related to performance. Within the baseline model, the estimate for time point was close to 0 for all tasks except gaze following, for which it was mostly negative (reflecting the downward trend we saw in Figure \ref{fig:perfplot}).

For gaze following, the model including sex as a predictor was ranked highest: males were somewhat less likely to follow the experimenter's gaze. For quantity and switching, the rank model was rated highest with lower-ranking individuals showing better quantity discrimination or switching abilities. In the case of switching, however, the model results should be interpreted with caution. The low re-test correlations suggest that the task does not reliably measure the cognitive ability in question. Thus, the variation in performance that the model tries to explain might not have a cognitive origin and could equally well be due to factors we did not capture.  

```{r}
# sex_pred <- bind_rows(
# posterior_samples(bm_sex[1], pars = "b_sexm")%>%mutate(task = "causality"),
# posterior_samples(bm_sex[2], pars = "b_sexm")%>%mutate(task = "gaze_following"),
# posterior_samples(bm_sex[3], pars = "b_sexm")%>%mutate(task = "inference"),
# posterior_samples(bm_sex[4], pars = "b_sexm")%>%mutate(task = "quantity"),
# posterior_samples(bm_switch_sex, pars = "b_sexm")%>%mutate(task = "switching")
# )%>%
#   dplyr::rename(value = b_sexm)%>%
#   mutate(predictor = "sex (male)")
# 
# 
# rank_pred <- bind_rows(
# posterior_samples(bm_rank[1], pars = "b_rank")%>%mutate(task = "causality"),
# posterior_samples(bm_rank[2], pars = "b_rank")%>%mutate(task = "gaze_following"),
# posterior_samples(bm_rank[3], pars = "b_rank")%>%mutate(task = "inference"),
# posterior_samples(bm_rank[4], pars = "b_rank")%>%mutate(task = "quantity"),
# posterior_samples(bm_switch_rank, pars = "b_rank")%>%mutate(task = "switching")
# )%>%
#   dplyr::rename(value = b_rank)%>%
#   mutate(predictor = "rank")
# 
# age_pred <- bind_rows(
# posterior_samples(bm_age[1], pars = "b_age")%>%mutate(task = "causality"),
# posterior_samples(bm_age[2], pars = "b_age")%>%mutate(task = "gaze_following"),
# posterior_samples(bm_age[3], pars = "b_age")%>%mutate(task = "inference"),
# posterior_samples(bm_age[4], pars = "b_age")%>%mutate(task = "quantity"),
# posterior_samples(bm_switch_age, pars = "b_age")%>%mutate(task = "switching")
# )%>%
#   dplyr::rename(value = b_age)%>%
#   mutate(predictor = "age")
# 
# time_pred <- bind_rows(
# posterior_samples(bm_null[1], pars = "b_time_point")%>%mutate(task = "causality"),
# posterior_samples(bm_null[2], pars = "b_time_point")%>%mutate(task = "gaze_following"),
# posterior_samples(bm_null[3], pars = "b_time_point")%>%mutate(task = "inference"),
# posterior_samples(bm_null[4], pars = "b_time_point")%>%mutate(task = "quantity"),
# posterior_samples(bm_switch_null, pars = "b_time_point")%>%mutate(task = "switching")
# )%>%
#   dplyr::rename(value = b_time_point)%>%
#   filter(value > -3)%>%
#   mutate(predictor = "time_point")
# 
# pred <-  bind_rows(sex_pred,rank_pred,age_pred,time_pred)
# 
# saveRDS(pred,file = "saves/pred.rds")
 
pred <- readRDS(file = "saves/pred.rds")

pred_summary <- pred %>%
  group_by(task,predictor) %>% 
  mean_hdci(value)

pred_plot <-  ggplot(data = pred, aes(x = value , y = predictor ,fill = task)) +
  facet_grid(~task, scales = "free")+
  #geom_vline(data = forest.data.summary%>%filter(time_point == "Pooled Effect"), aes(xintercept = b_Intercept), color = "grey", size = 1) +
  #geom_vline(data = forest.data.summary%>%filter(time_point == "Pooled Effect"), aes(xintercept = .lower), color = "grey", linetype = 2) +
    #geom_vline(data = forest.data.summary%>%filter(time_point == "Pooled Effect"), aes(xintercept = .upper), color = "grey", linetype = 2) +
  #geom_segment(y = "Time point 1", yend = "Time point 1", x = -Inf, xend = Inf, color = "black", size = 0.5) +
  geom_vline(xintercept = 0, color = "black", size = .5, lty = 2) +
  geom_density_ridges(rel_min_height = 0.01,col = "black", scale = 1.5,
                      alpha = 0.5) +
  geom_pointintervalh(data = pred_summary, size = 1)+
  labs(x = "Model Estimate",
       y = element_blank()) +
  theme_minimal(base_size = 10)+
  guides(fill = F)+
  scale_fill_colorblind()

#ggsave(file = "predictors.png", width = 10, height = 3, scale = 1.5)
```

```{r predplot, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Posterior distribution for the test predictors for each task. Dots represent the mean of the distribution with 95\\% HDI. Samples for time point are drawn from the baseline model. For all models except switching, the estimates are given in link space. No samples are shown for the rearing model."}
pred_plot
```


# Discussion 

We tested the same sample of great apes repeatedly on five cognitive tasks. This design allowed us to address some pressing questions in primate cognition research: How stable is group-level performance in cognitive tasks? How reliable are the results of these tasks? How much do individual characteristics influence performance? Below we discuss the results in light of these questions. 

Performance was relatively stable for all tasks except gaze following. This result is somewhat surprising given that individuals were differentially reinforced in all tasks -- except gaze following. Furthermore, counterbalancing and positioning were exactly the same at each time point. Together, this creates a potentially ideal learning scenario. How can we interpret this lack of improvement in the tasks other than gaze following? One explanation could be that the different routes to solving the task constitute incompatible information sources. For example, in the case of causality, apes could spontaneously solve the task by inferring that the food caused the sound. Alternatively, they could learn that food is under the cup the experimenter touches whenever they hear a rattling sound. In principle, these two information sources could easily be integrated and supplement one another, resulting in improved performance over time. The absence of improvement could mean that apes rely on spontaneous inferences alone, thereby ignoring repeating contingencies. However, many alternative explanations are possible. For example, many apes in [masked] have had years of experience with the kind of tasks we included in the study. Thus, the absence of improvement might indicate that they already reached an individual performance maximum. The continuation of this project might help to shed light on these questions. For now, we may conclude that short term improvements based on learned arbitrary relations are unlikely to occur in great apes. In support of this, when primates learned arbitrary relations in previous studies, it typically took a very long time and an elaborate training regime [e.g. @allritz2016chimpanzees].   

Three out of five tasks showed acceptable levels of reliability. Importantly, reliability is independent of group-level performance (leaving aside floor and ceiling effects) [see @hedge2018reliability]. Here, we see such a pattern for inference: Group-level performance was consistently at chance level for every time point. On a group level, one would conclude that great apes did not make the inference in question. However, the task was highly reliable, suggesting that it accurately captured individual differences. Together with the observation that some individuals consistently performed at ceiling (see grey transparent lines in Figure \ref{fig:perfplot}), this suggests that the task is well suited to measure inferential abilities on an *individual* level. The opposite pattern holds for quantity. Here, group-level performance was consistently above chance, but individual differences were not very consistent. This suggests that variation was due to sources other than systematic differences between individuals. This phenomenon is quite common in the human adult cognitive literature [@hedge2018reliability]. It arises when experimental tasks (optimized for low variance in measurement) are used to study individual differences (requiring high variance in measurement). Taken together, we may recommend that researchers investigate the psychometric properties of an experimental task before they use it to study individual differences. When planning to study individual differences by relating measures to one another, researchers might be well advised to first study the reliability of these measures. Even though this takes considerable time and effort, it increases the chances of finding meaningful effects.

We did not find that one of the individual-level characteristics (age, sex, rank or rearing history) was consistently related to performance across tasks. A baseline model, predicting performance by time variables alone, was, on average, rated highest in the different model comparisons. The model including rank was rated highest for two tasks (quantity and switching). However, in the case of switching, this should be interpreted with caution in the light of low reliability of the task (see results section). Moving forward, we will explore additional predictors, to see if we do find some that are related to cognitive performance more broadly. For now, we may conclude that researchers should carefully select predictors based on theoretical considerations. Including them as a default or to control for potential effects might make models unnecessarily complex -- and might not even have the desired effect [see @westfall2016statistically].

For our analysis, we combined the data from all species, neglecting potential species differences. The reason is that the sample size for each species was too small to really differentiate individual- from species-level differences. This is a common problem in primate cognition research. Species-level inferences require data sets that are beyond the resources of individual labs. A promising way forward to overcome this limitation is the *ManyPrimates* project; a large-scale collaborative initiative established to create an infrastructure to support the pooling of resources across labs [@many2019establishing]. 

The data we have reported here are the first couple of waves in a longitudinal study which we hope to continue for at least one year. As part of it, we will record additional variables that might explain variation in cognitive performance such as social network data or live history variables (sickness, birth and death of group members, etc.). We hope that this project will contribute to our understanding of the dynamic nature of primate cognition.

```{r tab1, results="asis"}
comp_tab <- model_comp%>%
  select(task, model, waic, se_waic, weight)%>%
  mutate_if(is.numeric, round, 2)%>%
  mutate(Task = c("Causality", rep("",4),"Gaze following", rep("",4),"Inference", rep("",4),"Quantity", rep("",4),"Switching", rep("",4)))%>%
  dplyr::rename(WAIC = waic,
                SE = se_waic, 
                Weight = weight,
                Model = model)%>%
  select(Task, Model, WAIC, SE, Weight)


tab1 <- xtable::xtable(comp_tab, 
                       caption = "WAIC Scores and weights for each predictor model and task.")

print(tab1, type="latex", comment = F, table.placement = "H", include.rownames=FALSE)
```

\vspace{0.2em} \fbox{\parbox[b][][c]{7.3cm}{\centering Corresponding data and code are available at [masked for peer review]}} \vspace{0.2em} 

<!-- \vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering Corresponding data and code are available at\ \url{https://github.com/ccp-eva/laac}}} \vspace{1em} -->

# Acknowledgements

Masked for peer review

<!-- We thank Damilola Olaoba and Anna Wolff for data collection, Luke Maurits for helpful discussion about the analytical approach and the animal caretakers at the WKPRC for their support.  -->

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent




